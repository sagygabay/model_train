# Deep Learning Model Comparison Framework

This framework trains multiple deep learning models for binary image classification, logs performance metrics, and generates publication-ready visualizations for comparison.

## Directory Structure

```
.
├── config.json               # Main configuration for data paths, models, training params
├── style_config.yaml         # Configuration for plot styling (fonts, colors, etc.)
├── train_evaluate.py         # Script to train models and save raw results
├── visualize.py              # Script to generate plots from saved results
├── requirements.txt          # Python dependencies
├── data/                     # (Example) Directory for input data
│   ├── train/                # Training images
│   │   ├── center/
│   │   └── not_center/
│   ├── validation/           # Validation images
│   │   ├── center/
│   │   └── not_center/
│   └── test/                 # Test images (Optional, path configured in config.json)
│       ├── center/
│       └── not_center/
└── output/                   # Directory for all outputs (configurable)
    ├── config_used.json      # Copy of the config used for the run
    ├── training_evaluation.log # Log file for the training script
    ├── overall_training_summary.csv # CSV summary of all trained models (includes val & test metrics)
    ├── <model_name_1>/       # Results for model 1
    │   ├── best_model.pth
    │   ├── final_model.pth
    │   ├── model_summary.json
    │   ├── training_history_plot.png # Basic plot generated during training
    │   ├── best_model_validation_confusion_matrix.png # Confusion matrix on validation set
    │   ├── best_model_test_confusion_matrix.png     # Confusion matrix on test set (if test set used)
    │   └── results/
    │       ├── training_history.csv
    │       ├── best_model_validation_classification_report.csv # Report on validation set
    │       ├── best_model_validation_preds.npz               # Predictions on validation set
    │       ├── best_model_test_classification_report.csv     # Report on test set (if test set used)
    │       └── best_model_test_preds.npz                   # Predictions on test set (if test set used)
    ├── <model_name_2>/       # Results for model 2
    │   └── ...
    └── visualizations/         # Directory for plots generated by visualize.py
        ├── combined_train_val_acc.pdf/png
        ├── combined_train_val_loss.pdf/png
        ├── comparison_convergence_speed.pdf/png
        ├── comparison_model_complexity.pdf/png
        ├── comparison_training_time.pdf/png
        ├── combined_lr_schedule.pdf/png   # Only if LR changed
        ├── <model_name_1>_confusion_matrix.png # Confusion matrix (typically validation set, depends on visualize.py)
        ├── <model_name_2>_confusion_matrix.png
        ├── final_metrics_summary.csv           # Summary table including validation and test metrics
        ├── final_metrics_summary.md            # Markdown version of the summary table
        └── visualization.log       # Log file for the visualization script
```

## Setup

1.  **Clone/Download:** Get the project files.
2.  **Prepare Data Directories:** Create separate directories for your training, validation, and (optionally) test data. Inside each of these directories, create subdirectories named `center` and `not_center`. Place your corresponding images into these subdirectories.
    ```
    data/
    ├── train/
    │   ├── center/
    │   │   └── img1.png
    │   └── not_center/
    │       └── img2.png
    ├── validation/
    │   ├── center/
    │   │   └── img3.png
    │   └── not_center/
    │       └── img4.png
    └── test/         # Optional: for final evaluation
        ├── center/
        │   └── img5.png
        └── not_center/
            └── img6.png
    ```
3.  **Install Dependencies:** Install the required Python libraries. It's recommended to use a virtual environment.
    ```bash
    pip install -r requirements.txt
    ```

## Configuration

1.  **`config.json`**:
    *   `data.train_dir`: Update path to your training data directory (e.g., `"./train"`).
    *   `data.val_dir`: Update path to your validation data directory (e.g., `"./validation"`).
    *   `data.test_dir`: Update path to your test data directory (e.g., `"./test"`). If this directory doesn't exist or is empty, test evaluation will be skipped.
    *   `data.output_dir`: Specify where all results and logs should be saved (e.g., `"output"`).
    *   `model_selection.candidates`: Enable/disable models using the `"enabled": true/false` flag. Configure `input_size` if needed.
    *   Adjust training parameters like `epochs`, `batch_size`, `learning_rate`, `freeze_backbone`, `lr_scheduler`, `early_stopping`, etc., under `model_selection`.
2.  **`style_config.yaml`**: (Optional) Modify this file to customize the appearance of the plots generated by `visualize.py`. You can change fonts, colors (using palettes or custom hex codes per model), line styles, figure sizes, DPI, etc.

## Usage

1.  **Train Models:**
    Run the training script using your configuration file.
    ```bash
    python train_evaluate.py --config config.json
    ```
    *   This script iterates through the enabled models in `config.json`.
    *   For each model, it trains, validates during training (saving the best model based on validation performance), logs progress, and saves checkpoints (`best_model.pth`, `final_model.pth`).
    *   After training, it evaluates the **best model** on both the **validation set** and the **test set** (if `test_dir` is valid).
    *   Raw results (training history, validation/test predictions, classification reports, confusion matrices) are saved into a dedicated subdirectory within the main `output_dir` (e.g., `output/resnet50/`).
    *   A log file (`training_evaluation.log`) and an overall summary CSV (`overall_training_summary.csv` including validation and test metrics) are saved in the main `output_dir`.

2.  **Generate Visualizations:**
    After training is complete, run the visualization script. Point it to the main output directory where the model results were saved.
    ```bash
    python visualize.py --results_dir output --config style_config.yaml
    ```
    *   This script scans the specified `results_dir` for model subdirectories containing the necessary result files.
    *   It loads the raw data (including validation and potentially test results) and generates comparison plots (accuracy/loss curves, convergence speed, complexity, training time, LR schedule) and individual confusion matrices based on the styles defined in `style_config.yaml` (or the specified `--config` file).
    *   Plots are saved in vector (`.pdf` or `.svg`) and raster (`.png`) formats in a `visualizations` subdirectory (e.g., `output/visualizations/`).
    *   A final summary table (`final_metrics_summary.csv` and `.md`), including both validation and test metrics, is also generated in the `visualizations` directory.
    *   A log file (`visualization.log`) is saved in the `visualizations` directory.

## Outputs

*   **`output/<model_name>/`**: Contains model checkpoints, a basic training plot, validation/test confusion matrices, a model-specific summary JSON, and a `results/` subdirectory.
*   **`output/<model_name>/results/`**: Contains raw data files like `training_history.csv`, validation/test classification reports (`best_model_validation_classification_report.csv`, `best_model_test_classification_report.csv`), and validation/test predictions (`best_model_validation_preds.npz`, `best_model_test_preds.npz`).
*   **`output/visualizations/`**: Contains all publication-ready plots generated by `visualize.py` and the final metrics summary tables (including test metrics).
*   **Log files**: `training_evaluation.log` and `visualizations/visualization.log` provide detailed logs for debugging.
